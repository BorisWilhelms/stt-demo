{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Speaker Speech to Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle notwendige Pakete importieren (siehe requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "import whisper\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definieren welches Audio-Recording genutzt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"ldn-topic.wav\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pipeline\n",
    "\n",
    "speaker-diarization pipeline initialisieren. Dafuer wird eine Registierung bei dem Model und ein Access Token fuer Huggingface benoetigt:\n",
    "\n",
    "1. visit hf.co/pyannote/speaker-diarization and hf.co/pyannote/segmentation and accept user conditions (only if requested)\n",
    "2. visit hf.co/settings/tokens to create an access token (only if you had to go through 1.)\n",
    "\n",
    "Huggingface Token muss in Umgebungsvariable `HF_TOKEN` gesetzt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio \"parsen\"\n",
    "\n",
    "Audio file parsen. `diarization` beinhaltet die abschnitte und den Sprecher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = pipeline(audio_file, num_speakers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper Model laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whispermodel = whisper.load_model(\"base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio splitten und chunks zu text umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_wav(audio_file)\n",
    "i = 1\n",
    "\n",
    "f = open(\"transscript.txt\", \"a\")\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    newAudio = audio[turn.start*1000:turn.end*1000]\n",
    "    newAudio.export(f\"part{i}.wav\", format=\"wav\")\n",
    "    result = whispermodel.transcribe(f\"part{i}.wav\")\n",
    "    f.write(f'{speaker}:{result[\"text\"]}\\n')\n",
    "    print(f\"start={turn.start} stop={turn.end} speaker_{speaker}\")\n",
    "    i = i + 1\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
