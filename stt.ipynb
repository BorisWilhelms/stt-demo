{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Speaker Speech to Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle notwendige Pakete importieren (siehe requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "import whisper\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definieren welches Audio-Recording genutzt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"ldn-topic.wav\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pipeline\n",
    "\n",
    "speaker-diarization pipeline initialisieren. Dafuer wird eine Registierung bei dem Model und ein Access Token fuer Huggingface benoetigt:\n",
    "\n",
    "1. visit hf.co/pyannote/speaker-diarization and hf.co/pyannote/segmentation and accept user conditions (only if requested)\n",
    "2. visit hf.co/settings/tokens to create an access token (only if you had to go through 1.)\n",
    "\n",
    "Huggingface Token muss in Umgebungsvariable `HF_TOKEN` gesetzt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu117. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio \"parsen\"\n",
    "\n",
    "Audio file parsen. `diarization` beinhaltet die abschnitte und den Sprecher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = pipeline(audio_file, num_speakers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper Model laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "whispermodel = whisper.load_model(\"base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio splitten und chunks zu text umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=0.4978125 stop=21.0009375 speaker_SPEAKER_01\n",
      "start=21.0009375 stop=44.06906250000001 speaker_SPEAKER_00\n",
      "start=44.06906250000001 stop=59.948437500000004 speaker_SPEAKER_01\n"
     ]
    }
   ],
   "source": [
    "audio = AudioSegment.from_wav(audio_file)\n",
    "i = 1\n",
    "\n",
    "f = open(\"transscript.txt\", \"a\")\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    newAudio = audio[turn.start*1000:turn.end*1000]\n",
    "    newAudio.export(f\"part{i}.wav\", format=\"wav\")\n",
    "    result = whispermodel.transcribe(f\"part{i}.wav\")\n",
    "    f.write(f'{speaker}:{result[\"text\"]}\\n')\n",
    "    print(f\"start={turn.start} stop={turn.end} speaker_{speaker}\")\n",
    "    i = i + 1\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
